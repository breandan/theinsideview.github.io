---
layout: post
title: "Connor Leahy on GPT3, EleutherAI and AI Alignment"
category: podcast
permalink: connor
youtubeId: HAFoIRNiKYE
---

{% include youtubePlayer.html id=page.youtubeId %}

In the first part of the podcast we chat about how to speed up GPT-3 training, how Conor updated on recent announcements of large language models, why GPT-3 is AGI for some [specific definitions of AGI](https://youtu.be/HrV19SjKUss?t=4785), the obstacles in plugging planning to GPT-N and why the brain might approximate something like backprop. We end this first chat with [solomonoff priors](https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference), adversarial attacks such as [Pascal Mugging](https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities), and whether direct work on AI Alignment is currently tractable. In the second part, we chat about his current projects at [EleutherAI](https://www.eleuther.ai/), multipolar scenarios and reasons to work on technical AI Alignment research.